{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple regression experiments\n",
    "\n",
    "using ray.tune for parallelization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# from luminous_plugin_dir import set_luminous_params\n",
    "from datetime import datetime\n",
    "import tensorflow.keras as keras\n",
    "from ray import tune\n",
    "import ray\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# set_luminous_params(log_level=PluginLogLevel.WARNING)\n",
    "\n",
    "def accum_dict_to_df(lcurves):\n",
    "\n",
    "    df_list = []\n",
    "    for key,value in lcurves.items():\n",
    "\n",
    "        df=pd.DataFrame(value).applymap(lambda x: x.numpy())\n",
    "        df[\"epoch\"] = df.index+1\n",
    "        df[\"accum_bits\"] = key\n",
    "        df_list.append(df)\n",
    "\n",
    "    df_lc = pd.concat(df_list, ignore_index=True)\n",
    "    return df_lc\n",
    "\n",
    "\n",
    "\n",
    "def shallow_linear(num_inputs, suffix=''):\n",
    "        inputs = keras.Input(shape=(num_inputs,), name=\"features\"+suffix)\n",
    "        outputs = layers.Dense(1, name=\"prediction\"+suffix)(inputs)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "\n",
    "def deep_linear(num_inputs, hidden = None, suffix='', activation=None):\n",
    "    hidden = hidden or [128]\n",
    "    inputs = keras.Input(shape=(num_inputs,), name=\"features\"+suffix)\n",
    "    x = inputs\n",
    "    for num_units in hidden:\n",
    "        x = layers.Dense(num_units, activation=activation or None)(x)\n",
    "    outputs = layers.Dense(1, name=\"prediction\"+suffix)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def create_model(learning_rate, batch_size):\n",
    "    import numpy as np\n",
    "    import os\n",
    "#     os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "   \n",
    "    \n",
    "    num_inputs = 8\n",
    "    wts_scale = 1\n",
    "    err_scale = 1e-2\n",
    "    \n",
    "    hidden = [4]*8\n",
    "    act_fn = 'gelu'\n",
    "    steps_per_epoch = 1\n",
    "    \n",
    "\n",
    "    #data params\n",
    "    num_train = 0\n",
    "    num_eval = 5000\n",
    "    \n",
    "    np.random.seed(222)\n",
    "    tf.random.set_seed(222)\n",
    "    Xdata = tf.random.normal(shape=(num_train+num_eval,num_inputs), dtype=tf.float32)\n",
    "\n",
    "\n",
    "    model_fn = lambda : deep_linear(num_inputs, hidden=hidden, activation=act_fn)\n",
    "    \n",
    "    \n",
    "    #create reference model and ground truth data\n",
    "    with tf.device(\"cpu\"):\n",
    "     \n",
    "        ref_model = model_fn()\n",
    "        ref_model.compile()\n",
    "        ref_weights = ref_model.get_weights()\n",
    "        ref_init = [wts_scale*np.float32(np.random.randn(*x.shape)) for x in ref_weights]\n",
    "        \n",
    "        \n",
    "        ref_model.set_weights(ref_init)\n",
    "        ydata = ref_model(Xdata)\n",
    "       \n",
    "        model_wts_init = [x + err_scale*np.float32(np.random.randn(*x.shape)) for x in ref_init]\n",
    "    \n",
    "#     with tf.device(\"cpu\"):\n",
    "\n",
    "        # create trained model\n",
    "        model = model_fn()\n",
    "        \n",
    "\n",
    "       # Instantiate an optimizer.\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        # Instantiate a loss function.\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=['mse']\n",
    "        )\n",
    "        # run fit 1x for tf-plugin to work correctly\n",
    "        X = np.zeros(shape=(1, num_inputs), dtype = np.float32)\n",
    "        Y = np.array([0], dtype=np.float32)\n",
    "        model.fit(X, Y, epochs=1, verbose=1)\n",
    "\n",
    "\n",
    "        # train val split      \n",
    "        Xtrain = Xdata[0:num_train,:]\n",
    "        ytrain = ydata[0:num_train,:]\n",
    "\n",
    "        Xval = Xdata[num_train::,:]\n",
    "        yval = ydata[num_train::,:]\n",
    "         # run inference for tf-plugin to work correctly\n",
    "        model(Xval, training=False)\n",
    "        \n",
    "        \n",
    "        def data_generator():\n",
    "            for i in range(steps_per_epoch):\n",
    "                x = tf.random.normal((batch_size, num_inputs), dtype=tf.float32)\n",
    "                y = tf.cast(ref_model(x),tf.float32)\n",
    "                yield x,y\n",
    "\n",
    "#         train_data = tf.data.Dataset.from_tensor_slices((Xtrain, ytrain))\n",
    "#         train_data = train_data.shuffle(1024).batch(batch_size)\n",
    "        train_data = tf.data.Dataset.from_generator(data_generator, (tf.float32, tf.float32))\n",
    "\n",
    "        train_mse_metric = tf.keras.metrics.MeanSquaredError()\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "                \n",
    "        val_data = (Xval, yval)\n",
    "        \n",
    "        model.set_weights(model_wts_init)\n",
    "        \n",
    "    return model, train_data, val_data\n",
    "\n",
    "\n",
    "class TuneReporterCallback(keras.callbacks.Callback):\n",
    "    \"\"\"Tune Callback for Keras.\n",
    "    \n",
    "    The callback is invoked every epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logs={}):\n",
    "        self.iteration = 0\n",
    "        super(TuneReporterCallback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.iteration += 1\n",
    "        tune.report(keras_info=logs, mean_mse=logs.get(\"mse\"),mean_loss=logs.get(\"loss\"))\n",
    "\n",
    "def tune_model(config):  # TODO: Change me.\n",
    "    from luminous_plugin import PluginLogLevel, set_luminous_params\n",
    "    model, train_data, val_data = create_model(config[\"lr\"], config[\"batch_size\"])\n",
    "   \n",
    "    \n",
    "    with tf.device('cpu'):\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            \"model.h5\", monitor='loss', save_best_only=True, save_freq=1)\n",
    "\n",
    "    # Enable Tune to make intermediate decisions by using a Tune Callback hook. This is Keras specific.\n",
    "   \n",
    "        callbacks = [checkpoint_callback, TuneReporterCallback()]\n",
    "    \n",
    "    set_luminous_params(bfloat16_acc_bits=config['accum_bits'],log_level=PluginLogLevel.WARNING)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        train_data, \n",
    "        validation_data=val_data,\n",
    "        verbose=0,  \n",
    "        epochs=50, \n",
    "        callbacks=callbacks)\n",
    "\n",
    "\n",
    "hyperparameter_space = {\n",
    "    \"lr\": tune.grid_search([1e-3]),  \n",
    "    \"batch_size\": tune.grid_search([10000]),\n",
    "    \"accum_bits\": tune.grid_search([24, 20, 18, 16]),\n",
    "}\n",
    "\n",
    "ray.shutdown()  # Restart Ray defensively in case the ray connection is lost. \n",
    "ray.init(log_to_driver=False)\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune_model, \n",
    "    verbose=1, \n",
    "    config=hyperparameter_space,\n",
    "    num_samples=1)\n",
    "\n",
    "df = pd.merge(pd.concat(analysis.trial_dataframes, ignore_index=True),analysis.dataframe(),on='experiment_id' )\n",
    "\n",
    "\n",
    "fig2=sns.relplot(data=df, x=\"training_iteration_x\", y=\"mean_mse_x\", style='config/batch_size',hue=\"config/accum_bits\", kind='line', height=10)\n",
    "fig2.set(yscale='log'), plt.grid(), plt.xlabel('Epochs'), plt.ylabel('Val MSE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2=sns.relplot(data=df, x=\"training_iteration_x\", y=\"mean_mse_x\",hue=\"config/lr\",style=\"config/batch_size\", kind='line', height=10)\n",
    "fig2.set(yscale='log'), plt.grid(), plt.xlabel('Epochs'), plt.ylabel('Val MSE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.ExperimentAnalysis('/root/ray_results/tune_model_2021-09-08_21-58-53/experiment_state-2021-09-08_21-58-53.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(pd.concat(analysis.trial_dataframes, ignore_index=True),analysis.dataframe(),on='experiment_id' )\n",
    "\n",
    "fig2=sns.relplot(data=df, x=\"training_iteration_x\", y=\"mean_mse_x\", style='config/batch_size',hue=\"config/accum_bits\", kind='line', height=10)\n",
    "fig2.set(yscale='log'), plt.grid(), plt.xlabel('Epochs'), plt.ylabel('Val MSE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat /root/ray_results/tune_model_2021-09-06_14-42-20/tune_model_a39e9_00004_4_accum_bits=12_2021-09-06_14-42-20/error.txt\n",
    "\n",
    "!cat /root/ray_results/tune_model_2021-09-07_19-27-51/tune_model_b12d5_00003_3_accum_bits=12_2021-09-07_19-27-51/error.txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2=sns.relplot(data=df, x=\"training_iteration_x\", y='mean_mse_x', style='config/batch_size',hue=\"config/accum_bits\", kind='line', height=10)\n",
    "fig2.set(yscale='log'), plt.grid(), plt.xlabel('Epochs'), plt.ylabel('Val MSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using custom training step, w/o parallelization (much slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from luminous_plugin_dir import set_luminous_params\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def accum_dict_to_df(lcurves):\n",
    "\n",
    "    df_list = []\n",
    "    for key,value in lcurves.items():\n",
    "\n",
    "        df=pd.DataFrame(value).applymap(lambda x: x.numpy())\n",
    "        df[\"epoch\"] = df.index+1\n",
    "        df[\"accum_bits\"] = key\n",
    "        df_list.append(df)\n",
    "\n",
    "    df_lc = pd.concat(df_list, ignore_index=True)\n",
    "    return df_lc\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#model params\n",
    "num_inputs = 8\n",
    "wts_scale = 3\n",
    "np.random.seed(113)\n",
    "# model_weights = wts_scale*np.float32(np.random.randn(num_inputs,1))\n",
    "# model_bias = wts_scale*np.float32(np.random.randn(1))\n",
    "\n",
    "hidden = [128]\n",
    "act_fn = \"linear\"\n",
    "\n",
    "err_scale = .001\n",
    "# wts_error_init = err_scale*np.float32(np.random.randn(num_inputs,1))\n",
    "# bias_error_init = err_scale*np.float32(np.random.randn(1))\n",
    "\n",
    "#data params\n",
    "num_train = 3000\n",
    "num_eval = 300\n",
    "tf.random.set_seed(111)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "\n",
    "def shallow_linear(num_inputs, suffix=''):\n",
    "    inputs = keras.Input(shape=(num_inputs,), name=\"features\"+suffix)\n",
    "    outputs = layers.Dense(1, name=\"prediction\"+suffix)(inputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def deep_linear(num_inputs, hidden = None, suffix='', activation=None):\n",
    "    hidden = hidden or [128]\n",
    "    inputs = keras.Input(shape=(num_inputs,), name=\"features\"+suffix)\n",
    "    x = inputs\n",
    "    for num_units in hidden:\n",
    "        x = layers.Dense(num_units, activation=activation or None)(x)\n",
    "    outputs = layers.Dense(1, name=\"prediction\"+suffix)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "Xdata = tf.random.normal(shape=(num_train+num_eval,num_inputs), dtype=tf.float32)\n",
    "with tf.device(\"cpu\"):\n",
    "    ref_model = deep_linear(num_inputs, hidden = hidden, activation = act_fn)\n",
    "    ref_weights = ref_model.get_weights()\n",
    "    ref_init = [wts_scale*np.float32(np.random.randn(*x.shape)) for x in ref_weights]\n",
    "    ref_model.set_weights(ref_init)\n",
    "    # ydata = tf.matmul(Xdata,model_weights) + model_bias\n",
    "    ref_model.compile()\n",
    "    ydata = ref_model(Xdata)\n",
    "    np.random.seed(11)\n",
    "    model_wts_init = [x + err_scale*np.float32(np.random.randn(*x.shape)) for x in ref_init]\n",
    "\n",
    "def exp_run(model_weights_init, accum_bits, num_train, batch_size, num_epochs, learning_rate):\n",
    "\n",
    "    with tf.device(\"cpu\"):\n",
    "\n",
    "\n",
    "        model = deep_linear(num_inputs,hidden, activation=act_fn)\n",
    "        model.set_weights(model_weights_init)\n",
    "            # Instantiate an optimizer.\n",
    "#         learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#             learning_rate,\n",
    "#             decay_steps=int(100000/batch_size),\n",
    "#             decay_rate=0.5,\n",
    "#             staircase=True)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        # Instantiate a loss function.\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "        )\n",
    "\n",
    "        X = np.zeros(shape=(1, num_inputs), dtype = np.float32)\n",
    "        Y = np.array([0], dtype=np.float32)\n",
    "        model.fit(X, Y, epochs=1, verbose=1)\n",
    "        \n",
    "        Xtrain = Xdata[0:num_train,:]\n",
    "        ytrain = ydata[0:num_train,:]\n",
    "\n",
    "        Xval = Xdata[num_train::,:]\n",
    "        yval = ydata[num_train::,:]\n",
    "\n",
    "        model(Xval, training=False)\n",
    "\n",
    "        train_data = tf.data.Dataset.from_tensor_slices((Xtrain, ytrain))\n",
    "        train_data = train_data.shuffle(num_train).batch(batch_size)\n",
    "\n",
    "        train_mse_metric = tf.keras.metrics.MeanSquaredError()\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "\n",
    "    set_luminous_params(bfloat16_acc_bits=accum_bits)\n",
    "\n",
    "    # Keep results for plotting\n",
    "    train_mse_results = []\n",
    "    val_mse_results = []\n",
    "    weights_mse = []\n",
    "    bias_mse = []\n",
    "    val_mae = []\n",
    "    grad_norms = []\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def max_abs_error(pred, truth):\n",
    "        return tf.reduce_max(tf.abs(pred-truth))\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x_batch, y_true):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            y_pred = model(x_batch, training=True)\n",
    "\n",
    "            batch_loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "            grads = tape.gradient(batch_loss, model.trainable_weights)\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        grad_norm = tf.norm(grads[0])\n",
    "\n",
    "        train_mse_metric.update_state(y_true, y_pred)\n",
    "        return batch_loss, grad_norm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(\"epoch\", epoch)\n",
    "        grad_norm_steps = []\n",
    "        for step, (x_batch, y_true) in enumerate(train_data):\n",
    "            batch_loss, grad_norm = train_step(x_batch, y_true)\n",
    "#             grad_norm_steps.append(grad_norm)\n",
    "\n",
    "\n",
    "        train_mse = train_mse_metric.result()\n",
    "        train_mse_results.append(train_mse)\n",
    "        train_mse_metric.reset_states()\n",
    "#         grad_norms.append(tf.reduce_mean(grad_norm_steps))\n",
    "\n",
    "    #     # Run a validation loop at the end of each epoch.\n",
    "\n",
    "\n",
    "        val_pred = model(Xval, training=False)\n",
    "        val_mse_results.append(mse(yval, val_pred))\n",
    "        val_mae.append(max_abs_error(yval, val_pred))\n",
    "\n",
    "#         weights_mse.append(mse(model_weights, model.weights[0]))\n",
    "#         bias_mse.append(mse(model_bias,model.weights[1]))\n",
    "\n",
    "    return {\"train_mse\": train_mse_results,\n",
    "            \"val_mse\": val_mse_results,\n",
    "#             \"weights_mse\": weights_mse,\n",
    "#             \"bias_mse\": bias_mse,\n",
    "            \"val_mae\": val_mae,\n",
    "#             \"grad_norm\": grad_norms\n",
    "           }, model.get_weights()\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "df_batch_list=[]\n",
    "for batch_size in [100]:\n",
    "    \n",
    "    df_list = []\n",
    "    for lr in [1e-4]:#[2e-7]:\n",
    "\n",
    "        lcurves = {}\n",
    "        final_weights ={}\n",
    "\n",
    "        for accum_bits in [24, 20, 16]:\n",
    "\n",
    "            \n",
    "            lcurves[accum_bits], final_weights[accum_bits] = exp_run(model_wts_init, \n",
    "                                                         accum_bits, \n",
    "                                                         num_train, \n",
    "                                                         batch_size, \n",
    "                                                         num_epochs,\n",
    "                                                         lr)\n",
    "\n",
    "        \n",
    "        df = accum_dict_to_df(lcurves)\n",
    "        df['learning_rate'] = lr\n",
    "        df_list.append(df)\n",
    "    \n",
    "    df_batch = pd.concat(df_list,ignore_index=True)\n",
    "    df_batch['batch_size']=batch_size\n",
    "    df_batch_list.append(df_batch)\n",
    "\n",
    "df_curves = pd.concat(df_batch_list,ignore_index=True)\n",
    "print('done.')  \n",
    "\n",
    "fig2=sns.relplot(data=df_curves, x=\"epoch\", y=\"val_mse\",hue=\"accum_bits\", kind='line', height=10)\n",
    "fig2.set(yscale='log'), plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-plugin)",
   "language": "python",
   "name": "tf-plugin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
