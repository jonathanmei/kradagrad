{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f0fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d750e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ab457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, encoder_widths, decoder_widths, act_fn=nn.ReLU(), out_fn=None):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        assert encoder_widths[-1] == decoder_widths[0], \"encoder output and decoder input dims must match\"\n",
    "\n",
    "        enc_layers = {}\n",
    "        for k in range(len(encoder_widths) - 1):\n",
    "            enc_layers[f\"enc_layer_{k}\"] = nn.Linear(encoder_widths[k], encoder_widths[k + 1])\n",
    "        self.enc_layers = nn.ModuleDict(enc_layers)\n",
    "\n",
    "        dec_layers = {}\n",
    "        for k in range(len(decoder_widths) - 1):\n",
    "            dec_layers[f\"dec_layer_{k}\"] = nn.Linear(decoder_widths[k], decoder_widths[k + 1])\n",
    "        self.dec_layers = nn.ModuleDict(dec_layers)\n",
    "\n",
    "        self.act_fn = act_fn\n",
    "        self.out_fn = out_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        for k in range(len(self.enc_layers)):\n",
    "            x = self.enc_layers[f'enc_layer_{k}'](x)\n",
    "            x = x if k == len(self.enc_layers) - 1 else self.act_fn(x)\n",
    "\n",
    "        for k in range(len(self.dec_layers)):\n",
    "            x = self.dec_layers[f'dec_layer_{k}'](x)\n",
    "            if k == len(self.dec_layers) - 1:\n",
    "                x = x if self.out_fn is None else self.out_fn(x)\n",
    "            else:\n",
    "                x = self.act_fn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09bada3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DenseNet(encoder_widths = [784, 200,100,8], decoder_widths = [8,100,200,784])\n",
    "\n",
    "net.enc_layers['enc_layer_2'](net.enc_layers['enc_layer_1'](net.enc_layers['enc_layer_0'](inputs))).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bf4e7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.003\n",
      "[1,   101] loss: 0.327\n",
      "[1,   201] loss: 0.308\n",
      "[1,   301] loss: 0.206\n",
      "[1,   401] loss: 0.124\n",
      "[1,   501] loss: 0.121\n",
      "[1,   601] loss: 0.123\n",
      "[1,   701] loss: 0.120\n",
      "[1,   801] loss: 0.118\n",
      "[1,   901] loss: 0.121\n",
      "[1,  1001] loss: 0.118\n",
      "[1,  1101] loss: 0.118\n",
      "[1,  1201] loss: 0.117\n",
      "[1,  1301] loss: 0.117\n",
      "[1,  1401] loss: 0.113\n",
      "[1,  1501] loss: 0.111\n",
      "[1,  1601] loss: 0.108\n",
      "[1,  1701] loss: 0.101\n",
      "[1,  1801] loss: 0.091\n",
      "[1,  1901] loss: 0.090\n",
      "[1,  2001] loss: 0.087\n",
      "[1,  2101] loss: 0.081\n",
      "[1,  2201] loss: 0.080\n",
      "[1,  2301] loss: 0.079\n",
      "[1,  2401] loss: 0.076\n",
      "[1,  2501] loss: 0.076\n",
      "[1,  2601] loss: 0.068\n",
      "[1,  2701] loss: 0.071\n",
      "[1,  2801] loss: 0.063\n",
      "[1,  2901] loss: 0.054\n",
      "[1,  3001] loss: 0.055\n",
      "[1,  3101] loss: 0.049\n",
      "[1,  3201] loss: 0.046\n",
      "[1,  3301] loss: 0.049\n",
      "[1,  3401] loss: 0.045\n",
      "[1,  3501] loss: 0.038\n",
      "[1,  3601] loss: 0.043\n",
      "[1,  3701] loss: 0.041\n",
      "[1,  3801] loss: 0.032\n",
      "[1,  3901] loss: 0.043\n",
      "[1,  4001] loss: 0.035\n",
      "[1,  4101] loss: 0.031\n",
      "[1,  4201] loss: 0.037\n",
      "[1,  4301] loss: 0.030\n",
      "[1,  4401] loss: 0.025\n",
      "[1,  4501] loss: 0.027\n",
      "[1,  4601] loss: 0.028\n",
      "[1,  4701] loss: 0.026\n",
      "[1,  4801] loss: 0.029\n",
      "[1,  4901] loss: 0.023\n",
      "[1,  5001] loss: 0.025\n",
      "[1,  5101] loss: 0.023\n",
      "[1,  5201] loss: 0.021\n",
      "[1,  5301] loss: 0.020\n",
      "[1,  5401] loss: 0.026\n",
      "[1,  5501] loss: 0.017\n",
      "[1,  5601] loss: 0.022\n",
      "[1,  5701] loss: 0.020\n",
      "[1,  5801] loss: 0.018\n",
      "[1,  5901] loss: 0.018\n",
      "[1,  6001] loss: 0.019\n",
      "[1,  6101] loss: 0.017\n",
      "[1,  6201] loss: 0.018\n",
      "[1,  6301] loss: 0.017\n",
      "[1,  6401] loss: 0.019\n",
      "[1,  6501] loss: 0.017\n",
      "[1,  6601] loss: 0.018\n",
      "[1,  6701] loss: 0.018\n",
      "[1,  6801] loss: 0.015\n",
      "[1,  6901] loss: 0.015\n",
      "[1,  7001] loss: 0.017\n",
      "[1,  7101] loss: 0.017\n",
      "[1,  7201] loss: 0.018\n",
      "[1,  7301] loss: 0.015\n",
      "[1,  7401] loss: 0.017\n",
      "[1,  7501] loss: 0.012\n",
      "[1,  7601] loss: 0.018\n",
      "[1,  7701] loss: 0.016\n",
      "[1,  7801] loss: 0.013\n",
      "[1,  7901] loss: 0.010\n",
      "[1,  8001] loss: 0.013\n",
      "[1,  8101] loss: 0.016\n",
      "[1,  8201] loss: 0.016\n",
      "[1,  8301] loss: 0.014\n",
      "[1,  8401] loss: 0.011\n",
      "[1,  8501] loss: 0.011\n",
      "[1,  8601] loss: 0.011\n",
      "[1,  8701] loss: 0.014\n",
      "[1,  8801] loss: 0.014\n",
      "[1,  8901] loss: 0.014\n",
      "[1,  9001] loss: 0.012\n",
      "[1,  9101] loss: 0.014\n",
      "[1,  9201] loss: 0.015\n",
      "[1,  9301] loss: 0.011\n",
      "[1,  9401] loss: 0.009\n",
      "[1,  9501] loss: 0.012\n",
      "[1,  9601] loss: 0.016\n",
      "[1,  9701] loss: 0.010\n",
      "[1,  9801] loss: 0.014\n",
      "[1,  9901] loss: 0.016\n",
      "[1, 10001] loss: 0.012\n",
      "[1, 10101] loss: 0.011\n",
      "[1, 10201] loss: 0.015\n",
      "[1, 10301] loss: 0.012\n",
      "[1, 10401] loss: 0.011\n",
      "[1, 10501] loss: 0.011\n",
      "[1, 10601] loss: 0.011\n",
      "[1, 10701] loss: 0.009\n",
      "[1, 10801] loss: 0.008\n",
      "[1, 10901] loss: 0.011\n",
      "[1, 11001] loss: 0.011\n",
      "[1, 11101] loss: 0.007\n",
      "[1, 11201] loss: 0.010\n",
      "[1, 11301] loss: 0.010\n",
      "[1, 11401] loss: 0.013\n",
      "[1, 11501] loss: 0.015\n",
      "[1, 11601] loss: 0.009\n",
      "[1, 11701] loss: 0.009\n",
      "[1, 11801] loss: 0.009\n",
      "[1, 11901] loss: 0.012\n",
      "[1, 12001] loss: 0.009\n",
      "[1, 12101] loss: 0.013\n",
      "[1, 12201] loss: 0.008\n",
      "[1, 12301] loss: 0.009\n",
      "[1, 12401] loss: 0.015\n",
      "[1, 12501] loss: 0.010\n",
      "[1, 12601] loss: 0.007\n",
      "[1, 12701] loss: 0.013\n",
      "[1, 12801] loss: 0.009\n",
      "[1, 12901] loss: 0.006\n",
      "[1, 13001] loss: 0.011\n",
      "[1, 13101] loss: 0.010\n",
      "[1, 13201] loss: 0.008\n",
      "[1, 13301] loss: 0.011\n",
      "[1, 13401] loss: 0.011\n",
      "[1, 13501] loss: 0.008\n",
      "[1, 13601] loss: 0.009\n",
      "[1, 13701] loss: 0.012\n",
      "[1, 13801] loss: 0.011\n",
      "[1, 13901] loss: 0.011\n",
      "[1, 14001] loss: 0.010\n",
      "[1, 14101] loss: 0.013\n",
      "[1, 14201] loss: 0.013\n",
      "[1, 14301] loss: 0.011\n",
      "[1, 14401] loss: 0.013\n",
      "[1, 14501] loss: 0.011\n",
      "[1, 14601] loss: 0.008\n",
      "[1, 14701] loss: 0.011\n",
      "[1, 14801] loss: 0.009\n",
      "[1, 14901] loss: 0.007\n",
      "[2,     1] loss: 0.009\n",
      "[2,   101] loss: 0.008\n",
      "[2,   201] loss: 0.007\n",
      "[2,   301] loss: 0.008\n",
      "[2,   401] loss: 0.008\n",
      "[2,   501] loss: 0.008\n",
      "[2,   601] loss: 0.011\n",
      "[2,   701] loss: 0.009\n",
      "[2,   801] loss: 0.007\n",
      "[2,   901] loss: 0.009\n",
      "[2,  1001] loss: 0.008\n",
      "[2,  1101] loss: 0.006\n",
      "[2,  1201] loss: 0.006\n",
      "[2,  1301] loss: 0.008\n",
      "[2,  1401] loss: 0.006\n",
      "[2,  1501] loss: 0.012\n",
      "[2,  1601] loss: 0.010\n",
      "[2,  1701] loss: 0.008\n",
      "[2,  1801] loss: 0.008\n",
      "[2,  1901] loss: 0.008\n",
      "[2,  2001] loss: 0.010\n",
      "[2,  2101] loss: 0.008\n",
      "[2,  2201] loss: 0.006\n",
      "[2,  2301] loss: 0.011\n",
      "[2,  2401] loss: 0.008\n",
      "[2,  2501] loss: 0.010\n",
      "[2,  2601] loss: 0.009\n",
      "[2,  2701] loss: 0.009\n",
      "[2,  2801] loss: 0.006\n",
      "[2,  2901] loss: 0.007\n",
      "[2,  3001] loss: 0.009\n",
      "[2,  3101] loss: 0.007\n",
      "[2,  3201] loss: 0.009\n",
      "[2,  3301] loss: 0.007\n",
      "[2,  3401] loss: 0.010\n",
      "[2,  3501] loss: 0.007\n",
      "[2,  3601] loss: 0.005\n",
      "[2,  3701] loss: 0.008\n",
      "[2,  3801] loss: 0.006\n",
      "[2,  3901] loss: 0.008\n",
      "[2,  4001] loss: 0.007\n",
      "[2,  4101] loss: 0.007\n",
      "[2,  4201] loss: 0.008\n",
      "[2,  4301] loss: 0.007\n",
      "[2,  4401] loss: 0.008\n",
      "[2,  4501] loss: 0.008\n",
      "[2,  4601] loss: 0.005\n",
      "[2,  4701] loss: 0.009\n",
      "[2,  4801] loss: 0.007\n",
      "[2,  4901] loss: 0.005\n",
      "[2,  5001] loss: 0.006\n",
      "[2,  5101] loss: 0.005\n",
      "[2,  5201] loss: 0.007\n",
      "[2,  5301] loss: 0.006\n",
      "[2,  5401] loss: 0.007\n",
      "[2,  5501] loss: 0.007\n",
      "[2,  5601] loss: 0.008\n",
      "[2,  5701] loss: 0.006\n",
      "[2,  5801] loss: 0.008\n",
      "[2,  5901] loss: 0.005\n",
      "[2,  6001] loss: 0.007\n",
      "[2,  6101] loss: 0.005\n",
      "[2,  6201] loss: 0.007\n",
      "[2,  6301] loss: 0.007\n",
      "[2,  6401] loss: 0.009\n",
      "[2,  6501] loss: 0.010\n",
      "[2,  6601] loss: 0.007\n",
      "[2,  6701] loss: 0.007\n",
      "[2,  6801] loss: 0.009\n",
      "[2,  6901] loss: 0.004\n",
      "[2,  7001] loss: 0.005\n",
      "[2,  7101] loss: 0.007\n",
      "[2,  7201] loss: 0.007\n",
      "[2,  7301] loss: 0.008\n",
      "[2,  7401] loss: 0.008\n",
      "[2,  7501] loss: 0.006\n",
      "[2,  7601] loss: 0.008\n",
      "[2,  7701] loss: 0.007\n",
      "[2,  7801] loss: 0.007\n",
      "[2,  7901] loss: 0.007\n",
      "[2,  8001] loss: 0.005\n",
      "[2,  8101] loss: 0.008\n",
      "[2,  8201] loss: 0.006\n",
      "[2,  8301] loss: 0.005\n",
      "[2,  8401] loss: 0.005\n",
      "[2,  8501] loss: 0.010\n",
      "[2,  8601] loss: 0.006\n",
      "[2,  8701] loss: 0.006\n",
      "[2,  8801] loss: 0.007\n",
      "[2,  8901] loss: 0.009\n",
      "[2,  9001] loss: 0.008\n",
      "[2,  9101] loss: 0.004\n",
      "[2,  9201] loss: 0.005\n",
      "[2,  9301] loss: 0.003\n",
      "[2,  9401] loss: 0.013\n",
      "[2,  9501] loss: 0.006\n",
      "[2,  9601] loss: 0.005\n",
      "[2,  9701] loss: 0.009\n",
      "[2,  9801] loss: 0.007\n",
      "[2,  9901] loss: 0.006\n",
      "[2, 10001] loss: 0.009\n",
      "[2, 10101] loss: 0.007\n",
      "[2, 10201] loss: 0.010\n",
      "[2, 10301] loss: 0.007\n",
      "[2, 10401] loss: 0.005\n",
      "[2, 10501] loss: 0.009\n",
      "[2, 10601] loss: 0.008\n",
      "[2, 10701] loss: 0.011\n",
      "[2, 10801] loss: 0.007\n",
      "[2, 10901] loss: 0.007\n",
      "[2, 11001] loss: 0.009\n",
      "[2, 11101] loss: 0.008\n",
      "[2, 11201] loss: 0.007\n",
      "[2, 11301] loss: 0.004\n",
      "[2, 11401] loss: 0.006\n",
      "[2, 11501] loss: 0.005\n",
      "[2, 11601] loss: 0.004\n",
      "[2, 11701] loss: 0.006\n",
      "[2, 11801] loss: 0.011\n",
      "[2, 11901] loss: 0.008\n",
      "[2, 12001] loss: 0.006\n",
      "[2, 12101] loss: 0.004\n",
      "[2, 12201] loss: 0.006\n",
      "[2, 12301] loss: 0.005\n",
      "[2, 12401] loss: 0.007\n",
      "[2, 12501] loss: 0.008\n",
      "[2, 12601] loss: 0.005\n",
      "[2, 12701] loss: 0.005\n",
      "[2, 12801] loss: 0.006\n",
      "[2, 12901] loss: 0.005\n",
      "[2, 13001] loss: 0.006\n",
      "[2, 13101] loss: 0.009\n",
      "[2, 13201] loss: 0.007\n",
      "[2, 13301] loss: 0.004\n",
      "[2, 13401] loss: 0.004\n",
      "[2, 13501] loss: 0.008\n",
      "[2, 13601] loss: 0.005\n",
      "[2, 13701] loss: 0.004\n",
      "[2, 13801] loss: 0.005\n",
      "[2, 13901] loss: 0.008\n",
      "[2, 14001] loss: 0.003\n",
      "[2, 14101] loss: 0.008\n",
      "[2, 14201] loss: 0.005\n",
      "[2, 14301] loss: 0.009\n",
      "[2, 14401] loss: 0.007\n",
      "[2, 14501] loss: 0.006\n",
      "[2, 14601] loss: 0.005\n",
      "[2, 14701] loss: 0.007\n",
      "[2, 14801] loss: 0.008\n",
      "[2, 14901] loss: 0.006\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "net = DenseNet(encoder_widths = [784, 200,100,8], decoder_widths = [8,100,200,784])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root=\"./data/mnist\", train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "running_loss = 0\n",
    "\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = nn.Flatten()(inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30eed27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 784])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.view(-1, 784).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34d1f867",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dec_layer_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mFlatten()(inputs)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[24], line 24\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_layers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(x)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_layers)):\n\u001b[0;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdec_layer_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m(x)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_layers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     26\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_fn(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:416\u001b[0m, in \u001b[0;36mModuleDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;129m@_copy_to_script_wrapper\u001b[39m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Module:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dec_layer_0'"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "    inputs, labels = data\n",
    "         \n",
    "    inputs = nn.Flatten()(inputs)\n",
    "\n",
    "    net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f8726b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "686f37ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f9e7ab95660>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18183acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
